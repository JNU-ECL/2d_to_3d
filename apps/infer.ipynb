{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import multiprocessing\n",
    "import os\n",
    "from importlib import import_module\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import temp_dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import platform\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import to_pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "args = easydict.EasyDict({\n",
    "    'batch_size':100,\n",
    "    'resize':(512,512),\n",
    "    'data_dir': '/dataset/mo2cap_dataset/test_data/test_data/olek_outdoor',\n",
    "    'dataset':'temp_dataset',\n",
    "    'model':'TempModel',\n",
    "    'model_dir': '/workspace/2d_to_3d/apps/exp370',\n",
    "    'output_dir': '/workspace/2d_to_3d/output'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(saved_model, device):\n",
    "    model_cls = getattr(import_module(\"model\"), args.model)\n",
    "    model = model_cls()\n",
    "\n",
    "    # tarpath = os.path.join(saved_model, 'best.tar.gz')\n",
    "    # tar = tarfile.open(tarpath, 'r:gz')\n",
    "    # tar.extractall(path=saved_model)\n",
    "\n",
    "    model_path = os.path.join(saved_model, 'best.pth')\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "class mo2cap_dataset(Dataset):\n",
    "\tdef __init__(self,data_dir,resize=(512,512)) -> None:\n",
    "\t\tself.data_dir = data_dir\n",
    "\t\tself.IMAGE = self._load_data(self.data_dir)\n",
    "\t\n",
    "\tdef _get_path(self,idx,datadict):\n",
    "\t\treturn datadict['data_paths'][idx]\n",
    "\t\n",
    "\tdef _load_data(self,dataroot):\n",
    "\t\textension = '*.jpg'\n",
    "\t\tpath_ = sorted(glob.glob(os.path.join(dataroot,extension)))\n",
    "\t\t\n",
    "\t\treturn {\n",
    "\t\t\t'data_paths' : path_,\n",
    "\t\t\t'len' : len(path_)\n",
    "\t\t}\n",
    "\t\n",
    "\tdef _get_rgba(self,img_path,resize=(512,512)):\n",
    "\t\timg = cv2.imread(img_path,cv2.IMREAD_COLOR)\n",
    "\t\timg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\t\timg = cv2.flip(img,0)\n",
    "\t\t# img = img[250:1000,:1280,:]\n",
    "\t\t# h,w,c = img.shape\n",
    "\t\t# c_h,c_w = h//2,w//2\n",
    "\t\t# radius = min(c_h,c_w) - 52\n",
    "\t\t# mask = np.zeros_like(img)\n",
    "\t\t# cv2.circle(mask, (c_w+40,c_h+52), radius, (255, 255, 255), -1)\n",
    "\t\th,w,c = img.shape\n",
    "\t\tc_h,c_w = h//2,w//2\n",
    "\t\tradius = 920//2\n",
    "\t\tmask = np.zeros_like(img)\n",
    "\t\tcv2.circle(mask, (c_w,c_h+110), radius, (255, 255, 255), -1)\n",
    "\t\tcropped_img = cv2.bitwise_and(img, mask)\n",
    "\t\tcropped_img = cropped_img[234:,:,:]\n",
    "\n",
    "\t\t# M = np.float32([[1, 0, 40], [0, 1, 0]])  # Transformation matrix for translation\n",
    "\t\t# translated_img = cv2.warpAffine(cropped_img, M, (1024, 800))\n",
    "\n",
    "\t\ttransform = transforms.Compose([\n",
    "\t\t\ttransforms.ToPILImage(),\n",
    "\t\t\ttransforms.Resize(resize, Image.BILINEAR),\n",
    "\t\t\ttransforms.ToTensor(),\n",
    "\t\t\t# transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "\t\t])\n",
    "\t\treturn transform(cropped_img)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.IMAGE['len']\n",
    "\t\n",
    "\tdef __getitem__(self, index):\n",
    "\t\timg_path = self._get_path(index,self.IMAGE)\n",
    "\t\timage = self._get_rgba(img_path)\n",
    "\t\treturn {'image':image}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_token = '/' if 'Linux' in platform.platform() else '\\\\'\n",
    "ktree_pred = [-1,  0,  0,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9,  9,  9, 12, 13,\n",
    "\t\t14, 16, 17, 18, 19, 20, 21, 15, 20, 25, 26, 20, 28, 29, 20, 31, 32,\n",
    "\t\t20, 34, 35, 20, 37, 38, 21, 40, 41, 21, 43, 44, 21, 46, 47, 21, 49,\n",
    "\t\t50, 21, 52, 53]\n",
    "\n",
    "ktree_label = [-1,0,0,0,1,2,3,4,5,6,7,8,9,12,12,13,14,15,16,17,18,12]\n",
    "xR_2_SMPL=[2,31,61,62,27,57,63,4,34,64,29,59,0,28,58,1,3,33,5,35,6,36,11,41]\n",
    "skip_num = []\n",
    "\n",
    "def depth_viewer(input_images, features):\n",
    "\ttemp_input_images=input_images.clone().detach().cpu()\n",
    "\ttemp_features=features.clone().detach().cpu()\n",
    "\t# temp_labels=labels.clone().detach().cpu()\n",
    "\tfig,ax = plt.subplots(len(input_images[:10]),2,figsize=(10, 70))\n",
    "\n",
    "\tfor i,(input_image, feature) in enumerate(zip(temp_input_images[0:100:10], temp_features[0:100:10])):\n",
    "\n",
    "\t\tinput_image=to_pil_image(input_image)\n",
    "\t\tfeature=to_pil_image(feature)\n",
    "\t\t# label=to_pil_image(label)\n",
    "\t\tax[i][0].set_title('input image')\n",
    "\t\tax[i][0].imshow(input_image )\n",
    "\t\tax[i][1].set_title('pred_depth_feature')\n",
    "\t\tax[i][1].imshow(feature)\n",
    "\t\t# ax[i][2].set_title('depth_GT')\n",
    "\t\t# ax[i][2].imshow(label)\n",
    "\n",
    "\n",
    "\n",
    "\treturn fig  \n",
    "\n",
    "\n",
    "def heatmap_viewer(input_images, heatmaps):\n",
    "\ttemp_input_images = input_images.clone().detach()\n",
    "\ttemp_heatmaps = heatmaps.clone().detach().cpu()\n",
    "\t# temp_labels = labels.clone().detach().cpu()\n",
    "\tfig, ax = plt.subplots(len(input_images[:10]),2,figsize=(10,70))\n",
    "\n",
    "\tfor i,(input_image, heatmap) in enumerate(zip(temp_input_images[0:100:10], temp_heatmaps[0:100:10])):\n",
    "\n",
    "\n",
    "\t\tinput_image = to_pil_image(input_image)\n",
    "\t\ttotal_heatmap = torch.zeros(heatmap.shape[1:])\n",
    "\t\tfor j in range(len(heatmap)):\n",
    "\t\t\ttotal_heatmap += heatmap[j,:,:]\n",
    "\n",
    "\t\t# total_label = torch.zeros(heatmap.shape[1:])\n",
    "\t\t# for j in range(len(label)):\n",
    "\t\t# \ttotal_label += label[j,:,:]\n",
    "\n",
    "\t\tax[i][0].set_title('image')\n",
    "\t\tax[i][0].imshow(input_image)\n",
    "\t\tax[i][1].set_title('pred_heatmap')\n",
    "\t\tax[i][1].imshow(total_heatmap)\n",
    "\t\t# ax[i][2].set_title('heatmap_GT')\n",
    "\t\t# ax[i][2].imshow(total_label)\n",
    "\n",
    "\n",
    "\treturn fig \n",
    "\n",
    "def joint_3d_viewer(input_images, joints):\n",
    "\ttemp_input_images = input_images.clone().detach().cpu()\n",
    "\t# temp_labels = labels.clone().detach().cpu()\n",
    "\ttemp_joints = joints.clone().detach().cpu()\n",
    "\tfig,ax = plt.subplots(len(input_images[:10]),2,figsize=(10,70),subplot_kw={\"projection\":\"3d\"})\n",
    "\tfor i in range(len(input_images[:10])):\n",
    "\t\trows, cols, start, stop = ax[i][0].get_subplotspec().get_geometry()\n",
    "\t\tax[i][0].remove()\n",
    "\t\tax[i][0] = fig.add_subplot(rows,cols,start+1)\n",
    "\t\n",
    "\tfor i,(input_image, joint) in enumerate(zip(temp_input_images[0:100:10], temp_joints[0:100:10])):\n",
    "\t\tinput_image=to_pil_image(input_image)\n",
    "\n",
    "\t\t# SMPL joints line plot\n",
    "\t\tfor j in reversed(range(len(joint))):\n",
    "\t\t\tif not j:break\n",
    "\t\t\tif j in skip_num : continue\n",
    "\t\t\tpred_joint_line_x=[joint[j,0],joint[ktree_label[j],0]]\n",
    "\t\t\tpred_joint_line_y=[joint[j,1],joint[ktree_label[j],1]]\n",
    "\t\t\tpred_joint_line_z=[joint[j,2],joint[ktree_label[j],2]]\n",
    "\t\t\tax[i][1].plot(pred_joint_line_x, pred_joint_line_y, pred_joint_line_z)\n",
    "\t\t\t# label_joint_line_x=[label[j,0],label[ktree_label[j],0]]\n",
    "\t\t\t# label_joint_line_y=[label[j,1],label[ktree_label[j],1]]\n",
    "\t\t\t# label_joint_line_z=[label[j,2],label[ktree_label[j],2]] \n",
    "\t\t\t# ax[i][2].plot(label_joint_line_x,label_joint_line_y, label_joint_line_z)\n",
    "\n",
    "\t\t# for j in reversed(range(len(label))):\n",
    "\t\t# \tif not j:break\n",
    "\t\t# \tif j in skip_num : continue\n",
    "\n",
    "\t\t# \tlabel_joint_line_x=[label[j,0],label[ktree_label[j],0]]\n",
    "\t\t# \tlabel_joint_line_y=[label[j,1],label[ktree_label[j],1]]\n",
    "\t\t# \tlabel_joint_line_z=[label[j,2],label[ktree_label[j],2]] \n",
    "\n",
    "\t\t# \tax[i][2].plot(label_joint_line_x,label_joint_line_y, label_joint_line_z)\n",
    "\t\t\n",
    "\t\tax[i][0].set_aspect('equal')\n",
    "\t\tax[i][1].set_aspect('equal')\n",
    "\t\t# ax[i][2].set_aspect('equal')\n",
    "\t\tax[i][1].view_init(-30,60,180)\n",
    "\t\t# ax[i][2].view_init(-30,60,180)\n",
    "\t\tax[i][1].set_xlabel('x')\n",
    "\t\tax[i][1].set_ylabel('y')\n",
    "\t\tax[i][1].set_zlabel('z')\n",
    "\t\t# ax[i][2].set_ylabel('y')\n",
    "\t\t# ax[i][2].set_xlabel('x')\n",
    "\t\t# ax[i][2].set_zlabel('z')\n",
    "\t\t\n",
    "\t\tax[i][0].imshow(input_image)\n",
    "\n",
    "\t\tax[i][0].set_title('image')\n",
    "\t\tax[i][1].set_title('pred_3d_joint')\n",
    "\t\t# ax[i][2].set_title('3d_joint_GT')\n",
    "\t# plt.show()\n",
    "\treturn fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def inference(args):\n",
    "\tuse_cuda = torch.cuda.is_available()\n",
    "\tdevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "\tmodel = load_model(args.model_dir, device).to(device)\n",
    "\tmodel = torch.nn.DataParallel(model,device_ids=[0,1])\n",
    "\tmodel.eval()\n",
    "\n",
    "\t# img_root = os.path.join(data_dir, 'images')\n",
    "\t# info_path = os.path.join(data_dir, 'info.csv')\n",
    "\t# info = pd.read_csv(info_path)\n",
    "\n",
    "\t# img_paths = [os.path.join(img_root, img_id) for img_id in info.ImageID]\n",
    "\t# dataset = temp_dataset(img_paths, args.resize)\n",
    "\n",
    "\t# dataset_module = getattr(import_module(\"dataset\"), args.dataset) \n",
    "\t# dataset = dataset_module(\n",
    "\t# \tdataroot=args.data_dir,\n",
    "\t# \tmode = 'test'\n",
    "\t# )\n",
    "\t\n",
    "\tdataset = mo2cap_dataset(args.data_dir)\n",
    "\n",
    "\tloader = torch.utils.data.DataLoader(\n",
    "\t\tdataset,\n",
    "\t\tbatch_size=args.batch_size,\n",
    "\t\t# num_workers=multiprocessing.cpu_count() // 2,\n",
    "\t\tnum_workers=0,\n",
    "\t\tshuffle=False,\n",
    "\t\tpin_memory=use_cuda,\n",
    "\t\tdrop_last=False,\n",
    "\t)\n",
    "\n",
    "\tprint(\"Calculating inference results..\")\n",
    "\tpreds = torch.tensor([])\n",
    "\t\n",
    "\tfor idx,batch in enumerate(tqdm(loader)):\n",
    "\t\tinputs = {'image' : batch['image'].to(device)}\n",
    "\t\tpred = model(inputs,is_train=False)\n",
    "\t\t\n",
    "\t\ttemp_joints = pred['regressor2_dict']['kp_3d_world_raw']\n",
    "\t\tindices = torch.tensor([12,21,19,17,16,18,20,2,5,8,11,1,4,7,10]).to(temp_joints.device)\n",
    "\t\ttemp_joints = torch.index_select(temp_joints,1,indices)\n",
    "\t\tpreds = torch.cat((preds.to(temp_joints.device),temp_joints),dim=0)\n",
    "\t\tif (idx+1)%1 == 0:\n",
    "\t\t\theat_fig = heatmap_viewer(inputs['image'],pred['heatmap'])\n",
    "\t\t\tdepth_fig = depth_viewer(inputs['image'],pred['depth_feature'])\n",
    "\t\t\tjoint_fig = joint_3d_viewer(inputs['image'],pred['regressor2_dict']['kp_3d_cam'])\n",
    "\t\t\theat_fig.savefig(f'/workspace/2d_to_3d/apps/eval_data/heat_fig_234/frame-{idx}.png')\n",
    "\t\t\tdepth_fig.savefig(f'/workspace/2d_to_3d/apps/eval_data/depth_fig_234/frame-{idx}.png')\n",
    "\t\t\tjoint_fig.savefig(f'/workspace/2d_to_3d/apps/eval_data/joint_fig_234/frame-{idx}.png')\n",
    "\t\t\tplt.close(heat_fig)\n",
    "\t\t\tplt.close(depth_fig)\n",
    "\t\t\tplt.close(joint_fig)\n",
    "\t\n",
    "\tprint(preds.shape)\n",
    "\tpreds_numpy = preds.cpu().numpy()\n",
    "\tnp.save('/workspace/2d_to_3d/apps/eval_data/olek_outdoor_preds_234.npy',preds_numpy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m gc\u001b[39m.\u001b[39mcollect()\n\u001b[1;32m      3\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[0;32m----> 4\u001b[0m \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntemp1 = torch.randn(10, 25, 3)\\n\\n# Define the indices you want to extract from the 1st dimension (size 25)\\nindices = torch.tensor([0, 2, 4, 6, 8, 10, 12, 14, 16, 18])\\n\\n# Extract the specified indices using the torch.index_select() function\\nsliced_tensor = torch.index_select(temp1, 1, indices)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "temp1 = torch.randn(10, 25, 3)\n",
    "\n",
    "# Define the indices you want to extract from the 1st dimension (size 25)\n",
    "indices = torch.tensor([0, 2, 4, 6, 8, 10, 12, 14, 16, 18])\n",
    "\n",
    "# Extract the specified indices using the torch.index_select() function\n",
    "sliced_tensor = torch.index_select(temp1, 1, indices)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating inference results..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28 [00:00<?, ?it/s]/tmp/ipykernel_150917/1520230202.py:46: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  transforms.Resize(resize, Image.BILINEAR),\n",
      "100%|██████████| 28/28 [03:46<00:00,  8.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2744, 15, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inference(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
