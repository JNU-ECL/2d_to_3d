{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.')\n",
    "from dataset import *\n",
    "from loss import create_criterion\n",
    "\n",
    "from model import get_pose_net\n",
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import multiprocessing\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import random\n",
    "import re\n",
    "from importlib import import_module\n",
    "from pathlib import Path\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import wandb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Optional, Dict, Union\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "import smplx\n",
    "\n",
    "from torchvision.transforms.functional import to_pil_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "args=easydict.EasyDict({\n",
    "    # Data and model checkpoints directories\n",
    "    'name':'exp',\n",
    "    'seed':42,\n",
    "    'epochs':5,\n",
    "    'dataset':'temp_dataset',\n",
    "    'augmentation':'BaseAugmentation', \n",
    "    'resize':[512,512], \n",
    "    'batch_size':20, \n",
    "    'valid_batch_size':64, \n",
    "    'model':'TempModel', \n",
    "    'optimizer':'Adam', \n",
    "    'log_interval':5,\n",
    "    'lr':0.001, \n",
    "    'val_ratio':0.2,\n",
    "    'criterion_1':'CustomLoss_joint',\n",
    "    'criterion_2':'Depth_loss',\n",
    "    'criterion_3':'2d_loss',\n",
    "    'criterion_4':'cam_loss',\n",
    "    'criterion_5':'3d_loss',\n",
    "    'criterion_6':'heatmap_loss',\n",
    "    'lr_decay_step':20, \n",
    "    'data_dir':r'F:\\ego_cam_dataset', \n",
    "    'model_dir':r'C:\\Users\\user\\Documents\\GitHub\\2d_to_3d\\apps',\n",
    "    'smpl_dir':r'D:\\SMPL\\SMPL_python_v.1.0.0\\smpl\\models'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "        \n",
    "def increment_path(path, exist_ok=False):\n",
    "    \"\"\" Automatically increment path, i.e. runs/exp --> runs/exp0, runs/exp1 etc.\n",
    "\n",
    "    Args:\n",
    "        path (str or pathlib.Path): f\"{model_dir}/{args.name}\".\n",
    "        exist_ok (bool): whether increment path (increment if False).\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    if (path.exists() and exist_ok) or (not path.exists()):\n",
    "        return str(path)\n",
    "    else:\n",
    "        dirs = glob.glob(f\"{path}*\")\n",
    "        matches = [re.search(rf\"%s(\\d+)\" % path.stem, d) for d in dirs]\n",
    "        i = [int(m.groups()[0]) for m in matches if m]\n",
    "        n = max(i) + 1 if i else 2\n",
    "        return f\"{path}{n}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_folder = r'C:\\Users\\user\\Documents\\GitHub\\smplx'\n",
    "model_type = 'smpl'\n",
    "plot_joints = 'true'\n",
    "use_face_contour = False\n",
    "gender = 'female'\n",
    "ext = 'npz'\n",
    "num_betas = 10\n",
    "num_expression_coeffs = 10\n",
    "ktree=[-1,  0,  0,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9,  9,  9, 12, 13,\n",
    "        14, 16, 17, 18, 19, 15, 15, 15, 20, 25, 26, 20, 28, 29, 20, 31, 32,\n",
    "        20, 34, 35, 20, 37, 38, 21, 40, 41, 21, 43, 44, 21, 46, 47, 21, 49,\n",
    "        50, 21, 52, 53]\n",
    "xR_2_SMPL=[2,31,61,62,27,57,63,4,34,64,29,59,0,28,58,1,3,33,5,35,6,36,11,41]\n",
    "skip_num = [13, 14, 15]\n",
    "\n",
    "\n",
    "def joint_2d_viewer(input_images, joints, labels, infos):\n",
    "    temp_input_images = input_images.clone().detach().cpu()\n",
    "    temp_joints = joints.clone().detach().cpu()\n",
    "    temp_labels = labels.clone().detach().cpu()\n",
    "    temp_infos = infos\n",
    "    fig,ax = plt.subplots(len(labels),3,figsize=(10, 70))\n",
    "   \n",
    "    for i,(input_image,pred_joint,label,info) in enumerate(zip(temp_input_images,temp_joints,temp_labels,temp_infos)):\n",
    "        # ax = fig.add_subplot(len(input_images),1,i)\n",
    "        # ax.scatter(joints[:22, 0], joints[:22, 1], -joints[:22, 2], color='r')\n",
    "\n",
    "        pred_joint = pred_joint.numpy()\n",
    "        label = label.squeeze(0).numpy()\n",
    "        # # SMPL joints line plot\n",
    "        for j in reversed(range(22)):\n",
    "            if not j:break\n",
    "            if j in skip_num : continue\n",
    "            pred_joint_line_x=[pred_joint[j,0],pred_joint[ktree[j],0]]\n",
    "            pred_joint_line_y=[pred_joint[j,1],pred_joint[ktree[j],1]]\n",
    "            label_joint_line_x=[label[j,0],label[ktree[j],0]]\n",
    "            label_joint_line_y=[label[j,1],label[ktree[j],1]]\n",
    "            ax[i][1].plot(pred_joint_line_x,pred_joint_line_y)\n",
    "            ax[i][2].plot(label_joint_line_x,label_joint_line_y)\n",
    "\n",
    "        ax[i][0].set_aspect('equal')\n",
    "        ax[i][1].set_aspect('equal')\n",
    "        ax[i][2].set_aspect('equal')\n",
    "        ax[i][1].view_init(120,-120,-30)\n",
    "        ax[i][2].view_init(120,-120,-30)\n",
    "        # ax[i][1].set_xlabel('x')\n",
    "        # ax[i][1].set_ylabel('y')\n",
    "        \n",
    "\n",
    "\n",
    "        input_image=to_pil_image(input_image)\n",
    "        ax[i][0].imshow(input_image)\n",
    "\n",
    "        ax[i][0].set_title(info.split('\\\\')[3]+info.split('\\\\')[-1])\n",
    "        ax[i][1].set_title('pred_2d_joint')\n",
    "        ax[i][2].set_title('joint_GT')\n",
    "    # plt.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def fisheye_joint_2d_viewer(input_images, joints, labels, infos, feature_size = (512,512)):\n",
    "    temp_input_images = input_images.clone().detach().cpu()\n",
    "    temp_joints = joints.clone().detach().cpu()\n",
    "    temp_labels = labels.clone().detach().cpu()\n",
    "\n",
    "    fig,ax = plt.subplots(len(labels),3,figsize=(10, 70))\n",
    "   \n",
    "    for i,(input_image,pred_joint,label,info) in enumerate(zip(temp_input_images,temp_joints,temp_labels,infos)):\n",
    "        # ax = fig.add_subplot(len(input_images),1,i)\n",
    "        # ax.scatter(joints[:22, 0], joints[:22, 1], -joints[:22, 2], color='r')\n",
    "        label = label.squeeze(0)\n",
    "        # SMPL_form_pred=[]\n",
    "        # SMPL_form_GT=[]\n",
    "        # for xR_idx in xR_2_SMPL:\n",
    "        #     SMPL_form_pred.append(pred_joint[xR_idx])\n",
    "        #     SMPL_form_GT.append(label[xR_idx])\n",
    "        # SMPL_form_pred=torch.stack(SMPL_form_pred)    \n",
    "        # SMPL_form_GT=torch.stack(SMPL_form_GT)\n",
    "        # pred_joint = SMPL_form_pred\n",
    "        # label = SMPL_form_GT\n",
    "        # # SMPL joints line plot\n",
    "        for j in reversed(range(22)):\n",
    "            if not j:break\n",
    "            if j in skip_num : continue\n",
    "            pred_joint_line_x=[pred_joint[j,0],pred_joint[ktree[j],0]]\n",
    "            pred_joint_line_y=[pred_joint[j,1],pred_joint[ktree[j],1]]\n",
    "            label_joint_line_x=[label[j,0],label[ktree[j],0]]\n",
    "            label_joint_line_y=[label[j,1],label[ktree[j],1]]\n",
    "            ax[i][1].plot(pred_joint_line_x,pred_joint_line_y)\n",
    "            ax[i][2].plot(label_joint_line_x,label_joint_line_y)\n",
    "\n",
    "        ax[i][0].set_aspect('equal')\n",
    "        ax[i][1].set_aspect('equal')\n",
    "        ax[i][2].set_aspect('equal')\n",
    "  \n",
    "        ax[i][1].set_xlim(0,feature_size[1])\n",
    "        ax[i][1].set_ylim(0,feature_size[0])\n",
    "        ax[i][2].set_xlim(0,feature_size[1])\n",
    "        ax[i][2].set_ylim(0,feature_size[0])\n",
    "        ax[i][1].invert_yaxis()\n",
    "        ax[i][2].invert_yaxis()\n",
    "        # ax[i][1].set_xlabel('x')\n",
    "        # ax[i][1].set_ylabel('y')\n",
    "        \n",
    "\n",
    "\n",
    "        input_image=to_pil_image(input_image)\n",
    "        ax[i][0].imshow(input_image)\n",
    "\n",
    "        ax[i][0].set_title(info.split('\\\\')[3]+info.split('\\\\')[-1])\n",
    "        ax[i][1].set_title('pred_fisheye_joint')\n",
    "        ax[i][2].set_title('fisheye_GT')\n",
    "\n",
    "\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def joint_3d_viewer(input_images, joints, labels, infos):\n",
    "    temp_input_images = input_images.clone().detach().cpu()\n",
    "    temp_labels = labels.clone().detach().cpu()\n",
    "    temp_joints = joints.clone().detach().cpu()\n",
    "    fig,ax = plt.subplots(len(labels),3,figsize=(10,70),subplot_kw={\"projection\":\"3d\"})\n",
    "    for i in range(len(labels)):\n",
    "        rows, cols, start, stop = ax[i][0].get_subplotspec().get_geometry()\n",
    "        ax[i][0].remove()\n",
    "        ax[i][0] = fig.add_subplot(rows,cols,start+1)\n",
    "    \n",
    "    for i,(input_image, joint, label, info) in enumerate(zip(temp_input_images, temp_joints, temp_labels, infos)):\n",
    "        input_image=to_pil_image(input_image)\n",
    "\n",
    "        # SMPL joints line plot\n",
    "        for j in reversed(range(22)):\n",
    "            if not j:break\n",
    "            if j in skip_num : continue\n",
    "            pred_joint_line_x=[joint[j,0],joint[ktree[j],0]]\n",
    "            pred_joint_line_y=[joint[j,1],joint[ktree[j],1]]\n",
    "            pred_joint_line_z=[joint[j,2],joint[ktree[j],2]]\n",
    "            label_joint_line_x=[label[j,0],label[ktree[j],0]]\n",
    "            label_joint_line_y=[label[j,1],label[ktree[j],1]]\n",
    "            label_joint_line_z=[label[j,2],label[ktree[j],2]] \n",
    "            ax[i][1].plot(pred_joint_line_x, pred_joint_line_y, pred_joint_line_z)\n",
    "            ax[i][2].plot(label_joint_line_x,label_joint_line_y, label_joint_line_z)\n",
    "        \n",
    "        ax[i][0].set_aspect('equal')\n",
    "        ax[i][1].set_aspect('equal')\n",
    "        ax[i][2].set_aspect('equal')\n",
    "        ax[i][1].view_init(120,-120,-30)\n",
    "        ax[i][2].view_init(120,-120,-30)\n",
    "        ax[i][1].set_xlabel('x')\n",
    "        ax[i][1].set_ylabel('y')\n",
    "        ax[i][1].set_zlabel('z')\n",
    "        ax[i][2].set_ylabel('y')\n",
    "        ax[i][2].set_xlabel('x')\n",
    "        ax[i][2].set_zlabel('z')\n",
    "        \n",
    "        ax[i][0].imshow(input_image)\n",
    "\n",
    "        ax[i][0].set_title(info.split('\\\\')[3]+info.split('\\\\')[-1])\n",
    "        ax[i][1].set_title('pred_3d_joint')\n",
    "        ax[i][2].set_title('3d_joint_GT')\n",
    "\n",
    "    return fig\n",
    "\n",
    "def depth_viewer(input_images, features, labels, infos):\n",
    "    temp_input_images=input_images.clone().detach().cpu()\n",
    "    temp_features=features.clone().detach().cpu()\n",
    "    temp_labels=labels.clone().detach().cpu()\n",
    "    fig,ax = plt.subplots(len(labels),3,figsize=(10, 70))\n",
    "\n",
    "    for i,(input_image, feature, label,info) in enumerate(zip(temp_input_images, temp_features, temp_labels,infos)):\n",
    "\n",
    "        input_image=to_pil_image(input_image)\n",
    "        feature=to_pil_image(feature)\n",
    "        label=to_pil_image(label)\n",
    "        ax[i][0].set_title(info.split('\\\\')[3]+info.split('\\\\')[-1])\n",
    "        ax[i][0].imshow(input_image)\n",
    "        ax[i][1].set_title('pred_depth_feature')\n",
    "        ax[i][1].imshow(feature)\n",
    "        ax[i][2].set_title('depth_GT')\n",
    "        ax[i][2].imshow(label)\n",
    "\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "    return fig  \n",
    "\n",
    "def heatmap_viewer(input_images, heatmaps, labels, infos):\n",
    "    temp_input_images = input_images.clone().detach()\n",
    "    temp_heatmaps = heatmaps.clone().detach().cpu()\n",
    "    temp_labels = labels.clone().detach().cpu()\n",
    "    fig, ax = plt.subplots(len(labels),3,figsize=(10,70))\n",
    "\n",
    "    for i,(input_image, heatmap, label, info) in enumerate(zip(temp_input_images, temp_heatmaps, temp_labels, infos)):\n",
    "\n",
    "\n",
    "        input_image = to_pil_image(input_image)\n",
    "        total_heatmap = torch.zeros(heatmap.shape[1:])\n",
    "        for j in range(len(heatmap)):\n",
    "            total_heatmap += heatmap[j,:,:]\n",
    "\n",
    "        total_label = torch.zeros(heatmap.shape[1:])\n",
    "        for j in range(len(label)):\n",
    "            total_label += label[j,:,:]\n",
    "\n",
    "        ax[i][0].set_title(info.split('\\\\')[3]+info.split('\\\\')[-1])\n",
    "        ax[i][0].imshow(input_image)\n",
    "        ax[i][1].set_title('pred_heatmap')\n",
    "        ax[i][1].imshow(total_heatmap)\n",
    "        ax[i][2].set_title('heatmap_GT')\n",
    "        ax[i][2].imshow(total_label)\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "    return fig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_dir, model_dir, args, logging=True):\n",
    "    seed_everything(args.seed)\n",
    "    if logging:wandb.init(project=\"2d to 3d\", entity=\"vhehduatks\")\n",
    "\n",
    "    save_dir = increment_path(os.path.join(model_dir, args.name))\n",
    "    print(save_dir)\n",
    "    os.makedirs(save_dir)\n",
    "    # -- settings\n",
    "\n",
    "\n",
    "    # -- dataset\n",
    "    dataset_module = getattr(import_module(\"dataset\"), args.dataset) \n",
    "    dataset = dataset_module(\n",
    "        dataroot=data_dir,\n",
    "    )\n",
    "    # num_classes = dataset.num_classes  # 18\n",
    "\n",
    "    # -- augmentation\n",
    "    # transform_module = getattr(import_module(\"dataset\"), args.augmentation)  # default: BaseAugmentation\n",
    "    # transform = transform_module(\n",
    "    #     resize=args.resize,\n",
    "    #     # mean=dataset.mean,\n",
    "    #     # std=dataset.std,\n",
    "    # )\n",
    "    # dataset.set_transform(transform)\n",
    "\n",
    "    # -- data_loader\n",
    "    train_set, val_set = dataset.split_dataset()\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_set,\n",
    "        batch_size=args.batch_size,\n",
    "        # num_workers=multiprocessing.cpu_count() // 2,\n",
    "        num_workers= 0,\n",
    "        shuffle=True,\n",
    "        pin_memory=use_cuda,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_set,\n",
    "        batch_size=args.valid_batch_size,\n",
    "        # num_workers=multiprocessing.cpu_count() // 2,\n",
    "        num_workers= 0,\n",
    "        shuffle=False,\n",
    "        pin_memory=use_cuda,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    # # -- feature_model\n",
    "    # feature_model = get_pose_net(True)\n",
    "    # feature_model = torch.nn.DataParallel(feature_model)\n",
    "\n",
    "    # -- reg_model\n",
    "    model_module = getattr(import_module(\"model\"), args.model)  # default: BaseModel\n",
    "    model = model_module(\n",
    "\n",
    "    ).to(device)\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # -- loss & metric\n",
    "    # smpl_criterion = create_criterion(args.criterion_1)\n",
    "    depth_criterion = create_criterion(args.criterion_2)   # MSE\n",
    "    projection_criterion = create_criterion(args.criterion_3) # MSE\n",
    "    cam_criterion = create_criterion(args.criterion_4) # MSE\n",
    "    joint_3d_criterion = create_criterion(args.criterion_5) # MSE\n",
    "    heatmap_criterion = create_criterion(args.criterion_6) # cross_entropy\n",
    "\n",
    "    # opt_module = getattr(import_module(\"torch.optim\"), args.optimizer)  # default: SGD\n",
    "    optimizer = torch.optim.Adam(\n",
    "        params=model.parameters(),\n",
    "        lr=args.lr,\n",
    "        weight_decay=5e-4\n",
    "    )\n",
    "    scheduler = StepLR(optimizer, args.lr_decay_step, gamma=0.5)\n",
    "\n",
    "    # -- logging\n",
    "    # logger = SummaryWriter(log_dir=save_dir)\n",
    "    if logging:wandb.config=vars(args)\n",
    "    # with open(os.path.join(save_dir, 'config.json'), 'w', encoding='utf-8') as f:\n",
    "    #     json.dump(vars(args), f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # best_val_acc = 0\n",
    "    best_val_loss = np.inf\n",
    "    for epoch in range(args.epochs):\n",
    "        # train loop\n",
    "        model.train()\n",
    "\n",
    "        loss_value = 0\n",
    "        matches = 0\n",
    "        for idx, train_batch in enumerate(train_loader):\n",
    "            total_loss={}\n",
    "            ret_dict_train = train_batch\n",
    "            inputs = ret_dict_train\n",
    "            # joint_2d_labels = ret_dict_train['joints_2d'].to(device)\n",
    "            joint_3d_labels = ret_dict_train['joints_3d'].to(device)\n",
    "            depth_labels = ret_dict_train['depth'].to(device)\n",
    "            cam_labels_trans,cam_labels_rot = ret_dict_train['camera_info']\n",
    "            infos = ret_dict_train['info']\n",
    "            fisheye_labels = ret_dict_train['fisheye_joints_2d'].to(device)\n",
    "            heatmap_labels = ret_dict_train['heatmap'].to(device)\n",
    "\n",
    "            pred_dict = model(inputs)\n",
    "            \n",
    "            depth_loss = depth_criterion(pred_dict['depth_feature'], depth_labels)\n",
    "            total_loss['depth_loss']= depth_loss * 100\n",
    "\n",
    "\n",
    "            if epoch > 0:\n",
    "                \n",
    "                heatmap_loss = heatmap_criterion(pred_dict['heatmap'],heatmap_labels)\n",
    "                total_loss['heatmap_loss'] = heatmap_loss * 100\n",
    "                cam_loss_trans = cam_criterion(pred_dict['regressor2_res_dict']['pred_cam_trans'],cam_labels_trans.to(device))\n",
    "                cam_loss_rot = cam_criterion(pred_dict['regressor2_res_dict']['pred_cam_rot'],cam_labels_rot.to(device))\n",
    "                total_loss['cam_loss'] = ((cam_loss_trans * 0.01) + (cam_loss_rot * 1))/2\n",
    "            if epoch > 1:\n",
    "                fisheye_projection_2d_loss = projection_criterion(pred_dict['regressor2_res_dict']['fisheye_kp_2d'],fisheye_labels)\n",
    "                total_loss['projection_2d_loss'] = fisheye_projection_2d_loss * 0.001\n",
    "\n",
    "                joint_3d_loss = joint_3d_criterion(pred_dict['regressor2_res_dict']['kp_3d'],joint_3d_labels)\n",
    "                total_loss['joint_3d_loss'] = joint_3d_loss * 0.01\n",
    "            # fisheye_cam_trans_loss = cam_criterion(pred_dict['regressor1_res_dict']['pred_cam_trans'],cam_labels_trans.to(device))\n",
    "            # fisheye_cam_rot_loss = cam_criterion(pred_dict['regressor1_res_dict']['pred_cam_rot'],cam_labels_rot.to(device))\n",
    "            # total_loss['fisheye_cam_loss'] = (fisheye_cam_trans_loss * 0.01 + fisheye_cam_rot_loss * 1)/2\n",
    "\n",
    "\n",
    "            loss = torch.stack(list(total_loss.values())).sum()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss_value += loss.item()\n",
    "            # matches += (preds == labels).sum().item()\n",
    "            if (idx + 1) % args.log_interval == 0:\n",
    "                # train_loss = loss_value / args.log_interval\n",
    "                # train_acc = matches / args.batch_size / args.log_interval\n",
    "                current_lr = get_lr(optimizer)\n",
    "                print(\"=======================================================================\")\n",
    "                for loss_name,val in total_loss.items():\n",
    "                    print(\n",
    "                        f\"Epoch[{epoch}/{args.epochs}]({idx + 1}/{len(train_loader)}) || \"\n",
    "                        f\"training loss : {loss_name} : {val:4.4} || lr {current_lr}\"\n",
    "                        # f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n",
    "                    )\n",
    "                    if logging:\n",
    "                        wandb.log({\n",
    "                            loss_name : val,\n",
    "                            \"lr\" : current_lr,\n",
    "                            \"Epoch\" : epoch    \n",
    "                            })\n",
    "                # viewer(outs)\n",
    "                if (idx + 1) % 200 == 0:\n",
    "                    # fig2 = viewer(outs,joint_labels,infos)\n",
    "                    depth_feature_fig = depth_viewer(inputs['image'],pred_dict['depth_feature'],depth_labels,infos)\n",
    "                    # joint_2d_fig = joint_2d_viewer(inputs['image'], pred_dict['regressor2_res_dict']['kp_2d'],joint_2d_labels,infos)\n",
    "                    # fisheye_2d_fig = fisheye_joint_2d_viewer(inputs['image'],pred_dict['regressor2_res_dict']['fisheye_kp_2d'],fisheye_labels,infos)\n",
    "                    heatmap_fig = heatmap_viewer(inputs['image'],pred_dict['heatmap'],heatmap_labels,infos)\n",
    "                    joint_3d_fig = joint_3d_viewer(inputs['image'],pred_dict['regressor2_res_dict']['kp_3d'],joint_3d_labels,infos)\n",
    "                    if logging:\n",
    "                        wandb.log({\n",
    "                            \"depth_feature_fig\":[wandb.Image(depth_feature_fig)],\n",
    "                            # \"joint_2d_fig_img\":[wandb.Image(joint_2d_fig)],\n",
    "                            # \"fisheye_2d_fig\":[wandb.Image(fisheye_2d_fig)],\n",
    "                            \"heatmap_fig\":[wandb.Image(heatmap_fig)],\n",
    "                            \"joint_3d_fig\":[wandb.Image(joint_3d_fig)],\n",
    "                        })\n",
    "                # logger.add_scalar(\"Train/loss\", train_loss, epoch * len(train_loader) + idx)\n",
    "                # logger.add_scalar(\"Train/accuracy\", train_acc, epoch * len(train_loader) + idx)\n",
    "\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        # val loop\n",
    "        with torch.no_grad():\n",
    "            print(\"Calculating validation results...\")\n",
    "            model.eval()\n",
    "          \n",
    "            for val_idx,val_batch in enumerate(val_loader):\n",
    "                total_loss={}\n",
    "                ret_dict_train = val_batch\n",
    "                inputs = ret_dict_train['image'].to(device)\n",
    "                # joint_2d_labels = ret_dict_train['joints_2d'].to(device)\n",
    "                joint_3d_labels = ret_dict_train['joints_3d'].to(device)\n",
    "                depth_labels = ret_dict_train['depth'].to(device)\n",
    "                cam_labels_trans,cam_labels_rot = ret_dict_train['camera_info']\n",
    "                infos = ret_dict_train['info']\n",
    "                fisheye_labels = ret_dict_train['fisheye_joints_2d'].to(device)\n",
    "                heatmap_labels = ret_dict_train['heatmap'].to(device)\n",
    "\n",
    "                pred_dict = model(inputs)\n",
    "\n",
    "                depth_loss = depth_criterion(pred_dict['depth_feature'], depth_labels)\n",
    "                total_loss['depth_loss']= depth_loss * 100\n",
    "\n",
    "                heatmap_loss = heatmap_criterion(pred_dict['heatmap'],heatmap_labels)\n",
    "                total_loss['heatmap_loss'] = heatmap_loss * 100\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    cam_loss_trans = cam_criterion(pred_dict['regressor2_res_dict']['pred_cam_trans'],cam_labels_trans.to(device))\n",
    "                    cam_loss_rot = cam_criterion(pred_dict['regressor2_res_dict']['pred_cam_rot'],cam_labels_rot.to(device))\n",
    "                    total_loss['cam_loss'] = ((cam_loss_trans * 0.01) + (cam_loss_rot * 1))/2\n",
    "\n",
    "                if epoch > 1:\n",
    "                    fisheye_projection_2d_loss = projection_criterion(pred_dict['regressor2_res_dict']['fisheye_kp_2d'],fisheye_labels)\n",
    "                    total_loss['projection_2d_loss'] = fisheye_projection_2d_loss * 0.001\n",
    "\n",
    "                    joint_3d_loss = joint_3d_criterion(pred_dict['regressor2_res_dict']['kp_3d'],joint_3d_labels)\n",
    "                    total_loss['joint_3d_loss'] = joint_3d_loss * 0.01\n",
    "\n",
    "\n",
    "\n",
    "                # fisheye_cam_trans_loss = cam_criterion(pred_dict['regressor1_res_dict']['pred_cam_trans'],cam_labels_trans.to(device))\n",
    "                # fisheye_cam_rot_loss = cam_criterion(pred_dict['regressor1_res_dict']['pred_cam_rot'],cam_labels_rot.to(device))\n",
    "                # total_loss['fisheye_cam_loss'] = (fisheye_cam_trans_loss * 0.01 + fisheye_cam_rot_loss * 1)/2\n",
    "\n",
    "          \n",
    "\n",
    "                loss = torch.stack(list(total_loss.values())).sum()\n",
    "\n",
    "                if (val_idx + 1) % args.log_interval == 0:\n",
    "         \n",
    "                    current_lr = get_lr(optimizer)\n",
    "                    print(\"=======================================================================\")\n",
    "                    for loss_name,value in total_loss.items():\n",
    "                        loss_name = 'val_'+loss_name\n",
    "                        print(\n",
    "                            f\"val_Epoch[{epoch}/{args.epochs}]({val_idx + 1}/{len(train_loader)}) || \"\n",
    "                            f\"val_training loss : {loss_name} : {value:4.4} || lr {current_lr}\"\n",
    "                            # f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n",
    "                        )\n",
    "                        if logging:\n",
    "                            wandb.log({\n",
    "                                loss_name : value,\n",
    "                                \"val_lr\" : current_lr,\n",
    "                                \"val_Epoch\" : epoch    \n",
    "                                })\n",
    "                   \n",
    "                    if (val_idx + 1) % 200 == 0:\n",
    "                        # fig2 = viewer(outs,joint_labels,infos)\n",
    "                        depth_feature_fig = depth_viewer(inputs['image'],pred_dict['depth_feature'],depth_labels,infos)\n",
    "                        # joint_2d_fig = joint_2d_viewer(inputs['image'], pred_dict['regressor2_res_dict']['kp_2d'],joint_2d_labels,infos)\n",
    "                        # fisheye_2d_fig = fisheye_joint_2d_viewer(inputs['image'],pred_dict['regressor2_res_dict']['fisheye_kp_2d'],fisheye_labels,infos)\n",
    "                        heatmap_fig = heatmap_viewer(inputs['image'],pred_dict['heatmap'],heatmap_labels,infos)\n",
    "                        joint_3d_fig = joint_3d_viewer(inputs['image'],pred_dict['regressor2_res_dict']['kp_3d'],joint_3d_labels,infos)\n",
    "                        if logging:\n",
    "                            wandb.log({\n",
    "                                \"val_depth_feature_fig\":[wandb.Image(depth_feature_fig)],\n",
    "                                # \"joint_2d_fig_img\":[wandb.Image(joint_2d_fig)],\n",
    "                                # \"fisheye_2d_fig\":[wandb.Image(fisheye_2d_fig)],\n",
    "                                \"val_heatmap_fig\":[wandb.Image(heatmap_fig)],\n",
    "                                \"val_joint_3d_fig\":[wandb.Image(joint_3d_fig)],\n",
    "                            })\n",
    "                        if best_val_loss>loss:\n",
    "                            torch.save(model.module.state_dict(), f\"{save_dir}\\\\best.pth\")\n",
    "                            best_val_loss = loss\n",
    "                        torch.save(model.module.state_dict(), f\"{save_dir}\\\\last.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\Documents\\GitHub\\2d_to_3d\\apps\\train.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/2d_to_3d/apps/train.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\ego_cam_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvhehduatks\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\user\\Documents\\GitHub\\2d_to_3d\\apps\\wandb\\run-20230403_142239-2osdne87</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/vhehduatks/2d%20to%203d/runs/2osdne87\" target=\"_blank\">autumn-bush-115</a></strong> to <a href=\"https://wandb.ai/vhehduatks/2d%20to%203d\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Documents\\GitHub\\2d_to_3d\\apps\\exp546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\smplx\\lib\\site-packages\\torchvision\\transforms\\transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================================\n",
      "Epoch[0/5](5/4006) || training loss : depth_loss : 18.71 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](10/4006) || training loss : depth_loss : 14.7 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](15/4006) || training loss : depth_loss : 10.95 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](20/4006) || training loss : depth_loss : 9.488 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](25/4006) || training loss : depth_loss : 9.817 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](30/4006) || training loss : depth_loss : 9.526 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](35/4006) || training loss : depth_loss : 8.405 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](40/4006) || training loss : depth_loss : 9.045 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](45/4006) || training loss : depth_loss : 10.12 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](50/4006) || training loss : depth_loss : 9.49 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](55/4006) || training loss : depth_loss : 9.189 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](60/4006) || training loss : depth_loss : 7.773 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](65/4006) || training loss : depth_loss : 8.439 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](70/4006) || training loss : depth_loss : 7.969 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](75/4006) || training loss : depth_loss : 9.147 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](80/4006) || training loss : depth_loss : 7.939 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](85/4006) || training loss : depth_loss : 8.894 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](90/4006) || training loss : depth_loss : 8.541 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](95/4006) || training loss : depth_loss : 10.08 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](100/4006) || training loss : depth_loss : 9.028 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](105/4006) || training loss : depth_loss : 10.15 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](110/4006) || training loss : depth_loss : 7.684 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](115/4006) || training loss : depth_loss : 8.384 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](120/4006) || training loss : depth_loss : 10.44 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](125/4006) || training loss : depth_loss : 7.815 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](130/4006) || training loss : depth_loss : 6.904 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](135/4006) || training loss : depth_loss : 7.623 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](140/4006) || training loss : depth_loss : 9.111 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](145/4006) || training loss : depth_loss : 7.521 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](150/4006) || training loss : depth_loss : 7.141 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](155/4006) || training loss : depth_loss : 8.776 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](160/4006) || training loss : depth_loss : 8.447 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](165/4006) || training loss : depth_loss : 8.05 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](170/4006) || training loss : depth_loss : 9.018 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](175/4006) || training loss : depth_loss : 7.515 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](180/4006) || training loss : depth_loss : 7.824 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](185/4006) || training loss : depth_loss : 7.322 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](190/4006) || training loss : depth_loss : 7.595 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](195/4006) || training loss : depth_loss : 8.41 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](200/4006) || training loss : depth_loss : 8.326 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](205/4006) || training loss : depth_loss :  8.6 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](210/4006) || training loss : depth_loss : 7.785 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](215/4006) || training loss : depth_loss : 7.401 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](220/4006) || training loss : depth_loss : 7.395 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](225/4006) || training loss : depth_loss : 6.377 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](230/4006) || training loss : depth_loss : 6.669 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](235/4006) || training loss : depth_loss : 6.245 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](240/4006) || training loss : depth_loss : 7.175 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](245/4006) || training loss : depth_loss : 6.312 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](250/4006) || training loss : depth_loss : 6.328 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](255/4006) || training loss : depth_loss : 7.066 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](260/4006) || training loss : depth_loss : 5.837 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](265/4006) || training loss : depth_loss : 5.486 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](270/4006) || training loss : depth_loss : 6.402 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](275/4006) || training loss : depth_loss : 6.189 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](280/4006) || training loss : depth_loss : 5.357 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](285/4006) || training loss : depth_loss : 5.542 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](290/4006) || training loss : depth_loss : 6.116 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](295/4006) || training loss : depth_loss : 7.249 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](300/4006) || training loss : depth_loss : 6.591 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](305/4006) || training loss : depth_loss : 8.018 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](310/4006) || training loss : depth_loss : 6.194 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](315/4006) || training loss : depth_loss : 5.242 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](320/4006) || training loss : depth_loss : 5.658 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](325/4006) || training loss : depth_loss : 5.922 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](330/4006) || training loss : depth_loss : 6.165 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](335/4006) || training loss : depth_loss : 6.165 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](340/4006) || training loss : depth_loss : 7.573 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](345/4006) || training loss : depth_loss : 5.938 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](350/4006) || training loss : depth_loss : 5.828 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](355/4006) || training loss : depth_loss : 5.889 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](360/4006) || training loss : depth_loss : 6.267 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](365/4006) || training loss : depth_loss : 6.828 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](370/4006) || training loss : depth_loss : 7.017 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](375/4006) || training loss : depth_loss : 6.123 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](380/4006) || training loss : depth_loss : 5.751 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](385/4006) || training loss : depth_loss : 6.273 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](390/4006) || training loss : depth_loss : 6.951 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](395/4006) || training loss : depth_loss : 6.374 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](400/4006) || training loss : depth_loss : 6.43 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](405/4006) || training loss : depth_loss : 6.944 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](410/4006) || training loss : depth_loss : 6.55 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](415/4006) || training loss : depth_loss : 5.686 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](420/4006) || training loss : depth_loss :  6.6 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](425/4006) || training loss : depth_loss : 6.572 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](430/4006) || training loss : depth_loss : 5.623 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](435/4006) || training loss : depth_loss : 6.469 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](440/4006) || training loss : depth_loss : 5.997 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](445/4006) || training loss : depth_loss : 6.411 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](450/4006) || training loss : depth_loss : 5.922 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](455/4006) || training loss : depth_loss : 7.817 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](460/4006) || training loss : depth_loss : 6.496 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](465/4006) || training loss : depth_loss : 5.626 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](470/4006) || training loss : depth_loss : 5.578 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](475/4006) || training loss : depth_loss : 6.158 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](480/4006) || training loss : depth_loss : 5.309 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](485/4006) || training loss : depth_loss : 5.452 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](490/4006) || training loss : depth_loss : 5.651 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](495/4006) || training loss : depth_loss : 5.169 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](500/4006) || training loss : depth_loss : 5.149 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](505/4006) || training loss : depth_loss : 5.024 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](510/4006) || training loss : depth_loss : 4.84 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](515/4006) || training loss : depth_loss : 5.147 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](520/4006) || training loss : depth_loss : 5.014 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](525/4006) || training loss : depth_loss : 4.657 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](530/4006) || training loss : depth_loss : 5.324 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](535/4006) || training loss : depth_loss : 5.285 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](540/4006) || training loss : depth_loss : 5.043 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](545/4006) || training loss : depth_loss : 4.717 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](550/4006) || training loss : depth_loss : 4.916 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](555/4006) || training loss : depth_loss : 5.849 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](560/4006) || training loss : depth_loss : 7.222 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](565/4006) || training loss : depth_loss : 4.695 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](570/4006) || training loss : depth_loss : 5.036 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](575/4006) || training loss : depth_loss : 5.03 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](580/4006) || training loss : depth_loss : 4.999 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](585/4006) || training loss : depth_loss : 5.233 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](590/4006) || training loss : depth_loss : 4.553 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](595/4006) || training loss : depth_loss : 4.96 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](600/4006) || training loss : depth_loss : 4.954 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](605/4006) || training loss : depth_loss : 4.857 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](610/4006) || training loss : depth_loss : 4.725 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](615/4006) || training loss : depth_loss : 5.113 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](620/4006) || training loss : depth_loss : 4.61 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](625/4006) || training loss : depth_loss : 5.457 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](630/4006) || training loss : depth_loss : 4.593 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](635/4006) || training loss : depth_loss : 4.106 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](640/4006) || training loss : depth_loss : 4.162 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](645/4006) || training loss : depth_loss : 4.981 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](650/4006) || training loss : depth_loss : 4.768 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](655/4006) || training loss : depth_loss : 4.862 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](660/4006) || training loss : depth_loss : 4.674 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](665/4006) || training loss : depth_loss : 4.687 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](670/4006) || training loss : depth_loss : 5.068 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](675/4006) || training loss : depth_loss : 5.662 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](680/4006) || training loss : depth_loss : 4.684 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](685/4006) || training loss : depth_loss : 4.364 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](690/4006) || training loss : depth_loss : 4.554 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](695/4006) || training loss : depth_loss : 5.151 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](700/4006) || training loss : depth_loss : 5.648 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](705/4006) || training loss : depth_loss : 4.501 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](710/4006) || training loss : depth_loss : 4.476 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](715/4006) || training loss : depth_loss : 5.022 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](720/4006) || training loss : depth_loss : 5.442 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](725/4006) || training loss : depth_loss : 4.651 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](730/4006) || training loss : depth_loss : 4.443 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](735/4006) || training loss : depth_loss : 5.148 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](740/4006) || training loss : depth_loss : 5.209 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](745/4006) || training loss : depth_loss : 5.572 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](750/4006) || training loss : depth_loss : 5.211 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](755/4006) || training loss : depth_loss : 5.18 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](760/4006) || training loss : depth_loss : 4.59 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](765/4006) || training loss : depth_loss : 4.927 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](770/4006) || training loss : depth_loss : 5.147 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](775/4006) || training loss : depth_loss : 4.515 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](780/4006) || training loss : depth_loss : 4.464 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](785/4006) || training loss : depth_loss : 4.485 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](790/4006) || training loss : depth_loss : 4.534 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](795/4006) || training loss : depth_loss : 4.399 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](800/4006) || training loss : depth_loss : 5.454 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](805/4006) || training loss : depth_loss : 4.392 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](810/4006) || training loss : depth_loss : 4.13 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](815/4006) || training loss : depth_loss : 4.815 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](820/4006) || training loss : depth_loss : 4.692 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](825/4006) || training loss : depth_loss : 4.599 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](830/4006) || training loss : depth_loss : 5.333 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](835/4006) || training loss : depth_loss : 4.629 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](840/4006) || training loss : depth_loss : 4.838 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](845/4006) || training loss : depth_loss : 5.666 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](850/4006) || training loss : depth_loss : 4.662 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](855/4006) || training loss : depth_loss : 4.617 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](860/4006) || training loss : depth_loss : 4.592 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](865/4006) || training loss : depth_loss : 4.177 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](870/4006) || training loss : depth_loss : 4.026 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](875/4006) || training loss : depth_loss : 4.899 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](880/4006) || training loss : depth_loss : 4.456 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](885/4006) || training loss : depth_loss : 4.165 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](890/4006) || training loss : depth_loss : 5.552 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](895/4006) || training loss : depth_loss : 4.634 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](900/4006) || training loss : depth_loss : 5.653 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](905/4006) || training loss : depth_loss : 4.913 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](910/4006) || training loss : depth_loss : 4.106 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](915/4006) || training loss : depth_loss : 5.361 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](920/4006) || training loss : depth_loss : 4.322 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](925/4006) || training loss : depth_loss : 4.755 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](930/4006) || training loss : depth_loss : 4.578 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](935/4006) || training loss : depth_loss : 3.946 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](940/4006) || training loss : depth_loss : 4.724 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](945/4006) || training loss : depth_loss : 6.69 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](950/4006) || training loss : depth_loss : 4.581 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](955/4006) || training loss : depth_loss : 4.562 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](960/4006) || training loss : depth_loss : 4.437 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](965/4006) || training loss : depth_loss : 5.226 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](970/4006) || training loss : depth_loss : 4.604 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](975/4006) || training loss : depth_loss : 4.466 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](980/4006) || training loss : depth_loss : 4.441 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](985/4006) || training loss : depth_loss : 4.846 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](990/4006) || training loss : depth_loss : 4.305 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](995/4006) || training loss : depth_loss : 4.945 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1000/4006) || training loss : depth_loss : 4.633 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1005/4006) || training loss : depth_loss : 4.419 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1010/4006) || training loss : depth_loss : 4.697 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1015/4006) || training loss : depth_loss : 4.857 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1020/4006) || training loss : depth_loss : 4.894 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1025/4006) || training loss : depth_loss : 4.229 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1030/4006) || training loss : depth_loss : 5.019 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1035/4006) || training loss : depth_loss : 4.669 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1040/4006) || training loss : depth_loss : 4.314 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1045/4006) || training loss : depth_loss : 4.032 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1050/4006) || training loss : depth_loss : 5.066 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1055/4006) || training loss : depth_loss : 4.966 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1060/4006) || training loss : depth_loss : 4.784 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1065/4006) || training loss : depth_loss : 4.549 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1070/4006) || training loss : depth_loss : 5.407 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1075/4006) || training loss : depth_loss : 4.364 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1080/4006) || training loss : depth_loss : 4.174 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1085/4006) || training loss : depth_loss : 4.389 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1090/4006) || training loss : depth_loss : 4.899 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1095/4006) || training loss : depth_loss : 5.271 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1100/4006) || training loss : depth_loss : 4.228 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1105/4006) || training loss : depth_loss : 4.896 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1110/4006) || training loss : depth_loss :  4.4 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1115/4006) || training loss : depth_loss : 4.959 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1120/4006) || training loss : depth_loss : 4.765 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1125/4006) || training loss : depth_loss : 4.104 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1130/4006) || training loss : depth_loss : 5.814 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1135/4006) || training loss : depth_loss : 5.267 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1140/4006) || training loss : depth_loss : 5.629 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1145/4006) || training loss : depth_loss : 4.289 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1150/4006) || training loss : depth_loss : 5.285 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1155/4006) || training loss : depth_loss : 4.596 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1160/4006) || training loss : depth_loss : 5.492 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1165/4006) || training loss : depth_loss : 4.746 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1170/4006) || training loss : depth_loss : 4.279 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1175/4006) || training loss : depth_loss : 4.853 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1180/4006) || training loss : depth_loss : 4.486 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1185/4006) || training loss : depth_loss : 4.596 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1190/4006) || training loss : depth_loss : 5.516 || lr 0.001\n",
      "=======================================================================\n",
      "Epoch[0/5](1195/4006) || training loss : depth_loss : 4.606 || lr 0.001\n"
     ]
    }
   ],
   "source": [
    "data_dir = args.data_dir\n",
    "model_dir = args.model_dir\n",
    "print(data_dir)\n",
    "\n",
    "train(data_dir, model_dir, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('smplx')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b6491936ae5e9232e741cb0cf3bc3536e77224e6d1117105f3d0df307ba3b0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
